---
layout:     post
title:      吴恩达机器学习
subtitle:   coursera机器学习课程笔记
date:       2019-01-12
author:     Raymond
header-img: img/home-bg-o.jpg
catalog: true
tags:
    - 机器学习
---

## Cost Function(代价函数)
在线性回归(Linear Regression)中，我们会得到一个h(x)来代表我们的线性回归函数，那么怎么来得知这个函数的好坏，是否能很好的拟合我们的数据，我们就用到代价函数这个概念
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115111524704.png)
根据代价函数的公式，
这个里面(xi,yi)为我们原来数据集的点，通过计算原本实际输出和我们h函数输出的差值来判断我们这个函数拟合的效果。采用方差再取1/2来尽可能减小一些极端数据对我们代价函数的影响。

## Gradient Descent(梯度下降)
我们通过代价函数能够得知我们线性回归函数的好坏，但是如果需要测试很多组函数，从中找出代价函数值最小的，需要花费的功夫太大，我们可以通过梯度下降的方式来找出最优解。
#### 思想
![](https://img-blog.csdnimg.cn/20190115152809658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjk3MDQ1Ng==,size_16,color_FFFFFF,t_70)
这幅图给出了一个有两个参数的回归方程的代价函数，梯度下降的思想是：我们从这副图的任意一个点出发，回顾四周，找到下降速度最快的那条路，然后按一定步长向下走，直到走到一个点，四周都比它高，这就得到了一个局部的最优解。
#### 算法
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115153428220.png)
这个算法从定义上还是很好理解的，对于每个参数，求出它的偏导数，α是learning rate,用来决定每次下降的步长，我们只需要按这个方法一直迭代下去，到某一时刻会达到收敛，这是便得到我们的最优解。
#### 几个注意事项
###### （1）同时更新所有参数
对于有很多个参数的代价函数，我们需要每一次同时更新所有参数，以防止更新后的参数对其他参数迭代产生影响
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115154155850.png)
利用局部变量保存更新结果，算出所有参数的一次更新后再赋值。
###### （2）步长的选择
这里的α不能太大，不然有可能在接近最低点时直接越过去，从而导致我们的算法不能收敛。当然也不需要特别小，这会让我们的算法变慢。因为越接近最低点时，我们的偏导数会越小，那么自然而然的我们的步长就慢慢变小，所以我们也不需要在靠近最低点时修改α来保证达到最低点。
###### （3）是否是全局最优解
如我们上图的代价函数，我们在不同出发点时会得到不同的最低点，得到的可能只是一个局部最优解，那么我们怎么保证得到的是全局的最优解？
经过证明，我们的线性回归的代价函数，一定是一个convex function,如下图
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115155224499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjk3MDQ1Ng==,size_16,color_FFFFFF,t_70)
所以对于一个线性回归的问题，我们不必担心算出的是不是最优解，因为它收敛于一个点，那么这个点一定是最优解。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115111458743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjk3MDQ1Ng==,size_16,color_FFFFFF,t_70)
#### 实用技巧
###### Feature Scaling(特征缩放)
如果我们的cost function有很多变量，而变量之间的差距比较大，我们在迭代过程中可能会发现一些变量已经收敛，而其他变量却还需要很多时间，这时可以利用特征缩放的思想，把变量的变化范围缩小，如把所有的变量都变为在-1到1之间变化。
###### Mean Normalization（归一化）
对每个变量，采用
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190115211835258.png)
的标准归一化，其中μ是每个变量的平均值，s是标准差。
